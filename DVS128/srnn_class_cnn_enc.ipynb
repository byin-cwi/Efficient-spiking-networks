{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('usr')"
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  },
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from spikingjelly.clock_driven import functional\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = 11\n",
    "from spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "dataset_dir ='./data'\n",
    "batch_size = 16\n",
    "split_by = 'number'\n",
    "T = 20\n",
    "normalization = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=DVS128Gesture(dataset_dir, train=True, use_frame=True, frames_num=T,\n",
    "                            split_by=split_by, normalization=normalization),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    pin_memory=True)\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=DVS128Gesture(dataset_dir, train=False, use_frame=True, frames_num=T,\n",
    "                            split_by=split_by, normalization=normalization),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    drop_last=False,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,labels = next(iter(train_data_loader))\n",
    "img.shape,labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "idx = 0\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1).set_title('frame: '+str(i*2))\n",
    "    plt.imshow(img[idx,i*2,1,:,:].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "idx = 2\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1).set_title('frame: '+str(i*2))\n",
    "    plt.imshow(img[idx,i*2,0,:,:].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    mp_img = F.max_pool2d(img[idx:idx+1,i*2,1,:,:],2,2)\n",
    "    print(np.max(mp_img[0].cpu().numpy()),np.mean(mp_img[0].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "surrograte_type = 'MG'\n",
    "print('gradient type: ', surrograte_type)\n",
    "\n",
    "torch.manual_seed(2020)\n",
    "np.random.seed(200)\n",
    "thresh = 0.5  # neuronal threshold\n",
    "b_j0 = 0.1  # neural threshold baseline\n",
    "R_m = 1  # membrane resistance\n",
    "lens = 0.5\n",
    "gamma = 0.5\n",
    "\n",
    "def gaussian(x, mu=0., sigma=.5):\n",
    "    return torch.exp(-((x - mu) ** 2) / (2 * sigma ** 2)) / torch.sqrt(2 * torch.tensor(math.pi)) / sigma\n",
    "\n",
    "# define approximate firing function\n",
    "\n",
    "class ActFun_adp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):  # input = membrane potential- threshold\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.gt(0).float()  # is firing ???\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):  # approximate the gradients\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        # temp = abs(input) < lens\n",
    "        scale = 6.0\n",
    "        hight = .15\n",
    "        if surrograte_type == 'G':\n",
    "            temp = torch.exp(-(input**2)/(2*lens**2))/torch.sqrt(2*torch.tensor(math.pi))/lens\n",
    "        elif surrograte_type == 'MG':\n",
    "            temp = gaussian(input, mu=0., sigma=lens) * (1. + hight) \\\n",
    "                - gaussian(input, mu=lens, sigma=scale * lens) * hight \\\n",
    "                - gaussian(input, mu=-lens, sigma=scale * lens) * hight\n",
    "        elif surrograte_type == 'MG1':\n",
    "            temp = gaussian(input, mu=0., sigma=lens) * (1. + hight) \\\n",
    "                - gaussian(input, mu=lens, sigma=scale * lens) * hight \n",
    "        elif surrograte_type == 'MG2':\n",
    "            temp = gaussian(input, mu=0., sigma=lens) * (1. + hight) \\\n",
    "                - gaussian(input, mu=-lens, sigma=scale * lens) * hight\n",
    "        elif surrograte_type =='linear':\n",
    "            temp = F.relu(1-input.abs())\n",
    "        elif surrograte_type == 'slayer':\n",
    "            temp = torch.exp(-5*input.abs())\n",
    "        return grad_input * temp.float() * gamma\n",
    "    \n",
    "    \n",
    "act_fun_adp = ActFun_adp.apply   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_update_adp(inputs, mem, spike, tau_adp, b, tau_m, dt=1, isAdapt=1):\n",
    "    alpha = torch.exp(-1. * dt / tau_m).cuda()\n",
    "    ro = torch.exp(-1. * dt / tau_adp).cuda()\n",
    "    if isAdapt:\n",
    "        beta = 1.84\n",
    "    else:\n",
    "        beta = 0.\n",
    "    b = ro * b + (1 - ro) * spike\n",
    "    B = b_j0 + beta * b\n",
    "\n",
    "    mem = mem * alpha + (1 - alpha) * R_m * inputs - B * spike * dt\n",
    "    inputs_ = mem - B\n",
    "    spike = act_fun_adp(inputs_)  # act_fun : approximation firing function\n",
    "    return mem, spike, B, b\n",
    "\n",
    "def output_Neuron(inputs, mem, tau_m, dt=1):\n",
    "    \"\"\"\n",
    "    The read out neuron is leaky integrator without spike\n",
    "    \"\"\"\n",
    "    alpha = torch.exp(-1. * dt / tau_m)\n",
    "    mem = mem *alpha +  (1-alpha)*inputs\n",
    "    return mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class spike_cnn(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,output_dim, kernel_size=5,strides=1,\n",
    "                 pooling_type = None,pool_size = 2, pool_strides =2,\n",
    "                 tauM = 20,tauAdp_inital =100, tau_initializer = 'normal',tauM_inital_std = 5,tauAdp_inital_std = 5,\n",
    "                 is_adaptive=1,device='cuda:0'):\n",
    "        \n",
    "        super(spike_cnn, self).__init__()\n",
    "        # input_size = [c,w,h]\n",
    "        self.input_size = input_size\n",
    "        self.input_dim = input_size[0]\n",
    "        self.output_dim = output_dim\n",
    "        self.is_adaptive = is_adaptive\n",
    "        self.device = device\n",
    "        \n",
    "        if pooling_type is not None: \n",
    "            if pooling_type =='max':\n",
    "                self.pooling = nn.MaxPool2d(kernel_size=pool_size, stride=pool_strides, padding=1)\n",
    "            elif pooling_type =='avg':\n",
    "                self.pooling = nn.AvgPool2d(kernel_size=pool_size, stride=pool_strides, padding=1)\n",
    "        else:\n",
    "            self.pooling = None\n",
    "        self.BN = nn.BatchNorm2d(output_dim)\n",
    "        self.conv= nn.Conv2d(self.input_dim,output_dim,kernel_size=kernel_size,stride=strides)\n",
    "        \n",
    "        self.output_size = self.compute_output_size()\n",
    "        \n",
    "        self.tau_m = nn.Parameter(torch.Tensor(self.output_size))\n",
    "        self.tau_adp = nn.Parameter(torch.Tensor(self.output_size))\n",
    "        \n",
    "        if tau_initializer == 'normal':\n",
    "            nn.init.normal_(self.tau_m,tauM,tauM_inital_std)\n",
    "            nn.init.normal_(self.tau_adp,tauAdp_inital,tauAdp_inital_std)\n",
    "    \n",
    "    def set_neuron_state(self,batch_size):\n",
    "        self.mem = torch.rand(batch_size,self.output_size[0],self.output_size[1],self.output_size[2]).to(self.device)\n",
    "        self.spike = torch.zeros(batch_size,self.output_size[0],self.output_size[1],self.output_size[2]).to(self.device)\n",
    "        self.b = (torch.ones(batch_size,self.output_size[0],self.output_size[1],self.output_size[2])*b_j0).to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self,input_spike):\n",
    "        d_input = self.conv(input_spike.float())\n",
    "        d_input = self.BN(d_input)\n",
    "        if self.pooling is not None: \n",
    "            d_input = self.pooling(d_input)\n",
    "        self.mem,self.spike,theta,self.b = mem_update_adp(d_input,self.mem,self.spike,self.tau_adp,self.b,self.tau_m,isAdapt=self.is_adaptive)\n",
    "        \n",
    "        return self.mem,self.spike\n",
    "    \n",
    "    def compute_output_size(self):\n",
    "        x_emp = torch.randn([1,self.input_size[0],self.input_size[1],self.input_size[2]])   \n",
    "        out = self.conv(x_emp)\n",
    "        if self.pooling is not None: out=self.pooling(out)\n",
    "        # print(self.name+'\\'s size: ', out.shape[1:])\n",
    "        return out.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_s(nn.Module):\n",
    "    def __init__(self,criterion):\n",
    "        super(RNN_s, self).__init__()\n",
    "        self.criterion = criterion\n",
    "\n",
    "        self.n = 128\n",
    "        dim = 128\n",
    "        self.dim = dim\n",
    "        self.dense_i = nn.Linear(dim*4*4,self.n,bias=False)\n",
    "        self.dense_i2r = nn.Linear(self.n,self.n,bias=False)\n",
    "        self.dense_r = nn.Linear(self.n,self.n,bias=False)\n",
    "        self.dense_o = nn.Linear(self.n,11,bias=False)\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(2,dim,3,padding=1),\n",
    "                                   nn.BatchNorm2d(dim),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2,2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(dim,dim,3,padding=1),\n",
    "                                   nn.BatchNorm2d(dim),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2,2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(dim,dim,3,padding=1),\n",
    "                                   nn.BatchNorm2d(dim),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2,2))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(dim,dim,3,padding=1),\n",
    "                                   nn.BatchNorm2d(dim),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2,2))\n",
    "        self.conv5 = nn.Sequential(nn.Conv2d(dim,dim,3,padding=1),\n",
    "                                   nn.BatchNorm2d(dim),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2,2))\n",
    "        \n",
    "        \n",
    "        self.dp = nn.Dropout(0.5)\n",
    "        self.tau_adp_i = nn.Parameter(torch.Tensor(self.n))\n",
    "        self.tau_adp_r = nn.Parameter(torch.Tensor(self.n))\n",
    "        self.tau_adp_o = nn.Parameter(torch.Tensor(11))\n",
    "        \n",
    "        self.tau_m_i = nn.Parameter(torch.Tensor(self.n))\n",
    "        self.tau_m_r = nn.Parameter(torch.Tensor(self.n))\n",
    "        self.tau_m_o = nn.Parameter(torch.Tensor(11))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.dense_r.weight)\n",
    "        nn.init.xavier_uniform_(self.dense_i.weight)\n",
    "        nn.init.xavier_uniform_(self.dense_i2r.weight)\n",
    "\n",
    "        # nn.init.kaiming_uniform_(self.conv1[0].weight)\n",
    "        # nn.init.xavier_uniform_(self.conv2[0].weight)\n",
    "        # nn.init.xavier_uniform_(self.conv3[0].weight)\n",
    "        # nn.init.xavier_uniform_(self.conv4[0].weight)\n",
    "        # nn.init.xavier_uniform_(self.conv5[0].weight)\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.conv1[0].weight)\n",
    "        nn.init.kaiming_uniform_(self.conv2[0].weight)\n",
    "        nn.init.kaiming_uniform_(self.conv3[0].weight)\n",
    "        nn.init.kaiming_uniform_(self.conv4[0].weight)\n",
    "        nn.init.kaiming_uniform_(self.conv5[0].weight)\n",
    "      \n",
    "\n",
    "        \n",
    "        nn.init.normal_(self.tau_adp_i,25,5)\n",
    "        nn.init.normal_(self.tau_adp_r,25,5)\n",
    "        nn.init.normal_(self.tau_adp_o,25,5)\n",
    "        \n",
    "        nn.init.normal_(self.tau_m_i,20,5)\n",
    "        nn.init.normal_(self.tau_m_r,20,5)\n",
    "        nn.init.normal_(self.tau_m_o,10,2)\n",
    "\n",
    "        # nn.init.constant_(self.tau_adp_i,20)\n",
    "        # nn.init.constant_(self.tau_adp_r,20)\n",
    "        # nn.init.constant_(self.tau_adp_o,10)\n",
    "        \n",
    "        # nn.init.constant_(self.tau_m_i,20)\n",
    "        # nn.init.constant_(self.tau_m_r,20)\n",
    "        # nn.init.constant_(self.tau_m_o,3)\n",
    "        \n",
    "        self.b_h = self.b_o = b_j0\n",
    "\n",
    "    def forward(self, input,labels=None,sub_length =5,output_type='integrator'):\n",
    "        b,s,c,h,w = input.shape\n",
    "        mem_layer1 = spike_layer1 = torch.zeros(b, self.n).cuda()\n",
    "        mem_layer2 = spike_layer2 = torch.zeros(b, self.n).cuda()\n",
    "        mem_layer3 = spike_layer3 = mem_output = torch.zeros(b, 11).cuda()\n",
    "        # print(self.conv1.output_size)\n",
    "\n",
    "        self.b_i = self.b_o=self.b_r = b_j0\n",
    "        output = torch.zeros(b, 11).cuda()\n",
    "        loss = 0\n",
    "        predictions = []\n",
    "        fr = []\n",
    "\n",
    "        input_ = input.reshape(b*s,c,h,w)\n",
    "        conv1_out = self.conv1(input_)\n",
    "        conv2_out = self.conv2(conv1_out)\n",
    "        conv3_out = self.conv3(conv2_out)\n",
    "        conv4_out = self.conv4(conv3_out)\n",
    "        conv5_out = self.conv5(conv4_out).reshape(b,s,self.dim,4,4)\n",
    "        for i in range(s):\n",
    "            input_x = conv5_out[:,i,:,:,:]\n",
    "            # input_x= input[:,i,:,:,:]\n",
    "\n",
    "            # conv1_out = self.conv1(input_x)\n",
    "            # conv2_out = self.conv2(conv1_out)\n",
    "            # conv3_out = self.conv3(conv2_out)\n",
    "            # conv4_out = self.conv3(conv3_out)\n",
    "            # conv5_out = self.conv3(conv4_out)\n",
    "  \n",
    "            # print(conv5_out.shape)\n",
    "            # snn_in = conv5_out.view(-1,self.dim*2*2)\n",
    "            snn_in = self.dp(input_x.view(-1,self.dim*4*4))\n",
    "\n",
    "            d1_output = self.dense_i(snn_in)+self.dense_r(spike_layer2)\n",
    "            mem_layer1, spike_layer1, theta_i, self.b_i = mem_update_adp(d1_output, mem_layer1, spike_layer1, self.tau_adp_i, self.b_i,self.tau_m_i)\n",
    "            r_input = self.dense_i2r(spike_layer1)\n",
    "            mem_layer2, spike_layer2, theta_r, self.b_r = mem_update_adp(r_input, mem_layer2, spike_layer2, self.tau_adp_r, self.b_r,self.tau_m_r)\n",
    "            o_input = self.dense_o(spike_layer2)\n",
    "            if output_type == 'adp-mem':\n",
    "                mem_layer3, spike_layer3, theta_o, self.b_o = mem_update_adp(o_input, mem_layer3, spike_layer3, self.tau_adp_o, self.b_o,self.tau_m_o)\n",
    "            elif output_type == 'integrator':\n",
    "                mem_layer3 = output_Neuron(o_input, mem_layer3, self.tau_m_o)\n",
    "            output = output+ mem_layer3\n",
    "            output = F.log_softmax(output,dim=-1)#\n",
    "            \n",
    "            # output_  = F.log_softmax(output,dim=1)\n",
    "            predictions.append(output.data.cpu().numpy())\n",
    "            \n",
    "            \n",
    "            fr.append([spike_layer1.detach().mean().cpu().numpy(),\n",
    "                        spike_layer2.detach().mean().cpu().numpy()])\n",
    "\n",
    "        if labels is not None and i > 5:\n",
    "            loss += self.criterion(output, labels[:,i])*(1+(i-5)/5)\n",
    "                # if i==s-1:\n",
    "                #     loss += self.criterion(output, labels[:,i])\n",
    "        predictions = torch.tensor(predictions)\n",
    "        return predictions, loss,np.array(fr)\n",
    "\n",
    "    def predict(self,input):\n",
    "        prediction, _= self.forward(input)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "criterion = nn.NLLLoss()#nn.CrossEntropyLoss()#\n",
    "model = RNN_s(criterion=criterion)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\",device)\n",
    "# model=torch.load('./models/0.958-MG.pth')\n",
    "# model=torch.load('./models/0.9583-24--relu-MG.pth')\n",
    "\n",
    "# model=torch.load('./models/0.9548-153--relu-MG.pth')\n",
    "model=torch.load('./models/0.9652-22--relu-MG.pth')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_loader,after_num_frames=0,is_show=0):\n",
    "    model.eval()\n",
    "    test_acc = 0.\n",
    "    sum_samples = 0\n",
    "    fr_list = []\n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "        images = images.view(-1, 20,2,128,128).to(device)\n",
    "        labels = labels.view(-1,1).repeat_interleave(20,dim=1).long().to(device)#labels.long().to(device)\n",
    "        predictions, _,fr = model(images)\n",
    "        _, predicted = torch.max(predictions.data, 2)\n",
    "        labels = labels.cpu()\n",
    "        predicted = predicted.cpu().t()\n",
    "        fr_list.append(fr)\n",
    "        # print(predicted.shape)\n",
    "        test_acc += (predicted[:,-1] == labels[:,-1]).sum()\n",
    "        \n",
    "        sum_samples = sum_samples + predicted.numel()/20\n",
    "        torch.cuda.empty_cache()\n",
    "    if is_show:\n",
    "        print('Mean FR: ',np.mean(fr_list),np.array(fr_list).mean(axis=(0,1)))\n",
    "        return test_acc.data.cpu().numpy() / sum_samples,np.mean(fr_list)\n",
    "    else:\n",
    "        return test_acc.data.cpu().numpy() / sum_samples\n",
    "\n",
    "def test_frame(data_loader,after_num_frames=0,is_show=0):\n",
    "    model.eval()\n",
    "    test_acc = 0.\n",
    "    sum_samples = 0\n",
    "    test_acc_classes = np.zeros((11,20))\n",
    "    test_acc_count = np.zeros((11,1))                                       \n",
    "    fr_list = []\n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "        images = images.view(-1, 20,2,128,128).to(device)\n",
    "        labels = labels.view(-1,1).repeat_interleave(20,dim=1).long().to(device)#labels.long().to(device)\n",
    "        predictions, _,fr = model(images)\n",
    "        _, predicted = torch.max(predictions.data, 2)\n",
    "        labels = labels.cpu()\n",
    "        predicted = predicted.cpu().t()\n",
    "        fr_list.append(fr)\n",
    "        \n",
    "        test_acc += (predicted[:,after_num_frames:] == labels[:,after_num_frames:]).float().mean(axis=0)\n",
    "        f_test = predicted[:,after_num_frames:] == labels[:,after_num_frames:]\n",
    "        for i in range(f_test.shape[0]):\n",
    "            tmp = labels[i,0]\n",
    "            test_acc_classes[tmp] += f_test[i].float().cpu().numpy()\n",
    "            test_acc_count[tmp] += 1\n",
    "        if i==1:\n",
    "            print(f_test.shape)\n",
    "        \n",
    "        sum_samples = sum_samples + predicted.numel()\n",
    "        torch.cuda.empty_cache()\n",
    "    if is_show:\n",
    "        print('Mean FR: ',np.mean(fr_list),np.array(fr_list).mean(axis=(0,1)))\n",
    "        return test_acc.data.cpu().numpy() / i,test_acc_classes/test_acc_count,np.mean(fr_list)\n",
    "    else:\n",
    "        return test_acc.data.cpu().numpy() / i,test_acc_classes/test_acc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_acc = test(test_data_loader,is_show=1)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc_,test_acc_classes = test_frame(test_data_loader)\n",
    "last_acc = np.mean(test_acc_classes[:,-1])\n",
    "print('last frame',last_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc_classes.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,loader,optimizer,scheduler=None,num_epochs=10,file_name='-relu-MG.pth'):\n",
    "    best_acc = .87\n",
    "    path = 'models/'  # .pth'\n",
    "    acc_list=[]\n",
    "    test_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_acc = 0\n",
    "        train_loss_sum = 0\n",
    "        sum_samples = 0\n",
    "        fr_list = []\n",
    "        for i, (images, labels) in enumerate(loader):\n",
    "            images = images.view(-1, 20,2,128,128).to(device)\n",
    "            labels = labels.view(-1,1).repeat_interleave(20,dim=1).long().to(device)#labels.long().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            predictions, train_loss,fr_ = model(images, labels)\n",
    "            _, predicted = torch.max(predictions.data, 2)\n",
    "            \n",
    "            train_loss.backward()\n",
    "            train_loss_sum += train_loss\n",
    "            fr_list.append(fr_)\n",
    "            optimizer.step()\n",
    "\n",
    "            labels = labels.cpu()\n",
    "            predicted = predicted.cpu().t()\n",
    "            train_acc += (predicted == labels).sum()\n",
    "            sum_samples = sum_samples + predicted.numel()\n",
    "            torch.cuda.empty_cache()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "        train_acc = train_acc.data.cpu().numpy() / sum_samples\n",
    "        valid_acc = test(test_data_loader)\n",
    "        \n",
    "        if valid_acc>best_acc:\n",
    "            best_acc = valid_acc\n",
    "            torch.save(model, path+str(best_acc)[:6]+'-'+str(epoch)+'-'+file_name)\n",
    "\n",
    "        test_list.append(valid_acc)\n",
    "        acc_list.append(train_acc)\n",
    "        if epoch%1==0:\n",
    "            fr_ = np.array(fr_list).mean(axis=(0,1))\n",
    "            print(fr_,best_acc)\n",
    "            print('epoch: {:3d}, Train Loss: {:.4f}, Train Acc: {:.4f},Valid Acc: {:.4f},fr: {:.4f}'.format(epoch,\n",
    "                                                                            train_loss_sum.item()/len(loader),\n",
    "                                                                            train_acc,valid_acc,mean(fr_)), flush=True)\n",
    "    return [acc_list,test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_params = []\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 1.e-3#3e-3\n",
    "cnn_param = [model.conv1[1].weight, model.conv1[1].bias, model.conv1[0].weight, model.conv1[0].bias,\n",
    "             model.conv2[1].weight, model.conv2[1].bias, model.conv2[0].weight, model.conv2[0].bias,\n",
    "             model.conv3[1].weight, model.conv3[1].bias, model.conv3[0].weight, model.conv3[0].bias,\n",
    "             model.conv4[1].weight, model.conv4[1].bias, model.conv4[0].weight, model.conv4[0].bias,\n",
    "             model.conv5[1].weight, model.conv5[1].bias, model.conv5[0].weight, model.conv5[0].bias,\n",
    "             ]\n",
    "\n",
    "base_params = [model.dense_i.weight,#model.dense_i.bias, \n",
    "               model.dense_o.weight, #model.dense_o.bias,\n",
    "               model.dense_r.weight, #model.dense_r.bias, \n",
    "               model.dense_i2r.weight,# model.dense_i2r.bias\n",
    "               ]+cnn_param\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': base_params},\n",
    "    {'params': model.tau_adp_i, 'lr': learning_rate * 2},\n",
    "    {'params': model.tau_adp_r, 'lr': learning_rate * 2},\n",
    "    {'params': model.tau_adp_o, 'lr': learning_rate * 2},\n",
    "    {'params': model.tau_m_i, 'lr': learning_rate * 2},\n",
    "    {'params': model.tau_m_r, 'lr': learning_rate * 2},\n",
    "    {'params': model.tau_m_o, 'lr': learning_rate * 2}],\n",
    "    lr=learning_rate)\n",
    "\n",
    "# scheduler = StepLR(optimizer, step_size=100, gamma=.5) \n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=64)\n",
    "\n",
    "# training network\n",
    "\n",
    "# with sechdual\n",
    "acc_list = train(model,train_data_loader,optimizer,scheduler,num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.tau_adp_i.mean(),model.tau_adp_i.std())\n",
    "print(model.tau_adp_r.mean(),model.tau_adp_r.std())\n",
    "print(model.tau_adp_o.mean(),model.tau_adp_o.std())\n",
    "print(model.tau_m_i.mean(),model.tau_m_i.std())\n",
    "print(model.tau_m_r.mean(),model.tau_m_r.std())\n",
    "print(model.tau_m_o.mean(),model.tau_m_o.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_list[1])\n",
    "# plt.plot(acc_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test(test_data_loader,is_show=1)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc_,test_acc_classes = test_frame(test_data_loader)\n",
    "last_acc = np.mean(test_acc_classes[:,-1])\n",
    "print('last frame',last_acc)\n",
    "print(test_acc_classes[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc_classes.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(11):\n",
    "    plt.plot(test_acc_classes[i,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}